# @package _global_
defaults:
  - workspace_config: dummy  # single, dummy or  vector
  # - dataset_config: dataset_0 #, dataset_1, dataset_2, dataset_3, dataset_4, dataset_5, dataset_300_1, dataset_300_2, dataset_300_3, dataset_300_4, dataset_300_5 
  - /agent_config: prodmp_transformer_agent  # <- this is defined in ./conf/agent_config/prodmp_transformer_agent.yaml
  - process_batch_config: prodmp
  - encoder_config: passthrough
  - override /agent_config/model_config/inner_model_config: prodmp_transformer # <- this is defined in ./conf/agent_config/model_config/inner_model_config/prodmp_transformer.yaml
  - _self_
  - override /hydra/launcher: submitit_slurm

hydra:
  mode: MULTIRUN  # needed for launcher to be used
  launcher:
    # launcher/cluster specific options
    partition: "dev_gpu_4"
    timeout_min: 20 # in minutes, maximum time on this queue
    gres: gpu:11  # one gpu allocated
    mem_per_gpu: 65536  # in MB
    additional_parameters:
      cpus-per-task: 8  # maybe more?
  sweeper:
    defaults:
      dataset_config: dataset_0 , dataset_1

# Set what should be used to determine if model performance increased in testing in environment
performance_metric: reward #end_point_deviation # can be anything that is returned in the dict of workspace.test_agent(agent) or in epoch_info
performance_direction: max #min # {min, max}

device: auto  # {auto, cpu, cuda}. auto will select cuda if available, cpu otherwise
t_obs: 6 #3
t_pred: 60 # 12
t_act: 30 # 12
predict_past: False

train_split: 0.9 # <- use 90% of the data for training and 10% for validation
dataset_fully_on_gpu: True # <- load the whole dataset onto the GPU to speed up training. If that is too much -> set to False (will load into RAM, and only load minibatches to GPU).
trajectory_dir: lamp

fixed_split: False # <- can be used to specify a fixed subset of demonstrations to use for training and validation
train_trajectory_dir: null
val_trajectory_dir: null

data_loader_config:
  shuffle: True
  pin_memory: False
  num_workers: 0 # TODO: cannot change num_workers
  batch_size: 1024


epochs: 600 #750

early_stopping: False
eval_in_env_after_epochs: 25 # 9999
num_trajectories_in_env: 50 # 100
save_distance: 25 # additionally to the best model, save model every n epochs



group_from_overrides: False
name_from_overrides: True
ignore_in_name:
  - group
  - lamp

wandb:
  entity: johannes-pfeil-karlsruhe-institute-of-technology # wandb-user
  project: lamp
  group: movement-primitive-diffusion-experiment
  mode: online # online, offline, disabled